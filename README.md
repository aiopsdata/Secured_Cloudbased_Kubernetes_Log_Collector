## Secured_Cloudbased_Kubernetes_Log_Collector
This collector is designed to collect real-time Pod Logs, Events, Prometheus Logs and Metrics generated by different kubernetes resources and store them into Cloud or Local based [Elasticsearch](https://www.elastic.co/guide/index.html). The stored data can be use for training different Machine Learning models based on your requirements.You can also use this collected data to check the health of your cluster and the resources that are running on that cluster.

### Architecture <br/>
**Architecture of Log Collector consist of five microservices(PODS):** <br/><br/>
[***Elasticsearch:***](https://www.elastic.co/guide/index.html) All the collected data is store in this database. During installation you can choose local based or cloud based Elasticsearch based on your requirement.<br/><br/>
[***Kibana:***](https://www.elastic.co/kibana/) This is used for viewing and performing different operations on data which is stored in Elasticsearch.<br/><br/>
***Log Collector:*** The job of Log Collector is to collect Pod Logs and Events generated by kubernetes system and store them in Elasticsearch database. Log Collector internally consist of two python scripts:
1. PodLog script: It will collect logs of all the running pods irrespective of their namespace and store them into Elasticsearch.
2. Events script: It will collect all the events happened in Kubernetes system and store them into Elasticsearch.

***Node-Exporter:*** Prometheus gives us verity of kubernetes cluster information and we manage deployment of prometheus using Node-Exporter.<br/><br/>
***Metriccollector:*** It is designed to collet Metrics and Prometheus data and store it into Elasticsearch. Metriccollector consist of two python scripts.
1. Metric script: It collects metric data generated by kuberentes cluster and store it into Elasticsearch.
2. Prometheus script: It fetches the data generated by prometheus and store it into Elasticsearch.

### Local Based Elasticsearch
<p align="center">
 <img src="https://user-images.githubusercontent.com/48379491/135843185-88c16cc2-0aff-44a1-a717-26a50f8b6cde.png" width="475" height="475">
</p>

 ### Cloud Based Elasticsearch
 <p align="center">
 <img src="https://user-images.githubusercontent.com/48379491/135843195-2a1013e8-999c-4cf5-a46b-1a3031cd6ae3.png" width="475" hight="475">
 </p>
 
**Different users have different requirements and based on their requirements we have designed our Log Collector.**
1. **Local Based Elasticsearch Without Security** (`./install.sh -n`) <br/>
     The components deployed using this option doesn't require username and password for accesing Elasticsearch and Kibana. The Elasticsearch will have two
     indices for Log Collector i.e podslog and eventslog and two for metriccollector i.e mtericlog and prometheuslog.  
2. **Local Based Elasticsearch With Security** (`./install.sh `) <br/>
     In this option you are require to give SYSTEM_NAME and YOUR_NAME at installation time. During installation you will get your creds (ES_USER and ES_PASSWORD) for Elasticsearch and Kibana. In this option all the collected data will be stored under YOUR_NAME index in Elasticsearch.
3. **Cloud Based Elasticsaarch** (`./install.sh -c`) <br/>
     In this option you are require to enter Elasticsearch CLOUD_ID, USER_NAME and PASSWORD along with that you also need to enter SYSTEM_NAME and YOUR_NAME. All the collected data will be stored under YOUR_NAME index and SYSTEM_NAME will be use to differentiate your data from others.


 
## :mag:Prerequisites
[**Docker**](https://docs.docker.com/engine/install/) <br/>
[**Kubernetes(Minikube)**](https://minikube.sigs.k8s.io/docs/start/)<br/>
[**Helm Version 3**](https://helm.sh/docs/intro/install/)<br/>
[**Python3**](https://www.geeksforgeeks.org/download-and-install-python-3-latest-version/)
## :zap: Installation
### :cloud: Cloud Based Elasticsearch Installation
Install agents for cloud based Elasticsearch.<br>
The following installation steps will deploy Agents and allow you to store data generated by them on your Cloud based Elasticsearch.

**Enter credential using Flags**
```bash 
./install.sh -c -i CLOUD_ID -u USER_NAME -p PASSWORD -s SYSTEM_NAME -u USER_NAME
```
**OR**<br><br>
**Enter credential during runtime**
```bash
./install.sh -c
```
SYSTEM_NAME allow you to differentiate data generated by your system with other data on Elasticsearch
```
System Name cannot be empty, please enter System Name : 
SYSTEM_NAME
```
Enter your cloud Elasticsearch_Id
```
Cloud ID cannot be empty, please enter cloud ID : 
https://ELASTICSEARCH_CLOUD_ID:PORT
```
Enter your cloud User_Name
```
Cloud User cannot be empty, please enter cloud User : 
USER_NAME
```
Enter your Elasticsearch Password
```
Cloud Password cannot be empty, please enter cloud Password : 
PASSWORD
```

<br><br>
### :robot: Local Based Elasticsearch Installation Without Security
```bash
./install.sh -n
```

<br><br>
### :computer: Local Elasticsearch Installation With Security
The following installation steps will deploy Logcollector, Metric, Node-expoter, Elasticsearch and Kibana Pods. It will also generate a **User_ID** and **Password** which will be use to authenticate Elasticsearch and Kibana.
**Enter inputs using Flags**
```bash
./install.sh -s SYSTEM_NAME -u USER_NAME
```
**OR**<br><br>
**Enter inputs during runtime**
```bash
./install.sh
```
SYSTEM_NAME allow you to differentiate data generated by your system with other data on Elasticsearch
```bash
System Name cannot be empty, please enter System Name : 
SYSTEM_NAME
```
USER_NAME will be use as a Elasticsearch Index, under which all generated data will be stored.
```bash
Enter your User name (In small cases): 
USER_NAME
```
## :umbrella:Uninstallation
```bash
./uninstall.sh
```

<br/><br/>
## :triangular_flag_on_post: Flags For Installing And Uninstalling Specific Component 
You can use all these Flags with `./uninstall.sh` script too.
<br/><br/>
For installing **Kibana**, you can user **-k** flag along with others flags
```bash
./install.sh -c -i CLOUD_ID -u USER_NAME -p PASSWORD -s SYSTEM_NAME -k
```
or
```bash
./install.sh -k
```
<br/>

For installing **Logcollector**, you can user  **-l**  flag along with other flags
```bash
./install.sh -c -i CLOUD_ID -u USER_NAME -p PASSWORD -s SYSTEM_NAME -l
```
or

```bash
./install.sh -l
```
<br/>

For installing **Metricollector**, you can user  **-m**  flag along with other flags
```bash
./install.sh -c -i CLOUD_ID -u USER_NAME -p PASSWORD -s SYSTEM_NAME -m
```
or
```bash
./install.sh -m
```
<br/><br/>
## :snowflake:Access Components After Deployment
Run ``` minikube ip ``` to get your minikube ip address, which will help you in accessing different components.
<br/>
**Access Elasticsearch**
```bash 
http://minikubeip:32000
```

**Access Kibana**
```bash 
http://minikubeip:32002
```

**Access Prometheus** 
<br/>
 For accessing prometheus you need to get the URL on which application is running then you need to forward the port to access it on your host system.
```bash 
export POD_NAME=$(kubectl get pods --namespace aiops -l "app=prometheus-node-exporter,release=node-exporter" -o jsonpath="{.items[0].metadata.name}")
```
```bash 
kubectl port-forward --namespace aiops $POD_NAME 9100
```
After above steps, prometheus will be available on.
```bash
http://localhost:9100
```
